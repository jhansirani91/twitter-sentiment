{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2510329,"sourceType":"datasetVersion","datasetId":1520310}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Necessary Library","metadata":{}},{"cell_type":"code","source":"!pip install pyspellchecker\n!pip install googletrans\n!pip install translatepy\n# IF USING TPU\n# !pip install nltk\n# !pip install TextBlob\n# !pip install wordcloud\n# !pip install spacy\n# !python3 -m spacy download en_core_web_sm\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:30:03.662024Z","iopub.execute_input":"2024-08-07T22:30:03.662328Z","iopub.status.idle":"2024-08-07T22:30:52.90194Z","shell.execute_reply.started":"2024-08-07T22:30:03.662302Z","shell.execute_reply":"2024-08-07T22:30:52.900579Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport seaborn as sns\nimport math\n\nimport re\nimport string\nimport random\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob\nfrom spellchecker import SpellChecker\nfrom wordcloud import WordCloud\n# from googletrans import Translator\nfrom translatepy import Translator\nimport string\nimport random\n\n# download some NLTK data\nimport nltk\nfrom nltk.corpus import wordnet\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('brown')\nnltk.download('omw-1.4')\nnltk.download('wordnet2022')\n\n# Ensure nltk wordnet data is downloaded\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnltk.download('punkt')\n\n\n# Load the spaCy model\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\n# Copy files from wordnet2022 to wordnet\n! cp -rf /usr/share/nltk_data/corpora/wordnet2022 /usr/share/nltk_data/corpora/wordnet # temp fix for lookup error.\n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\nimport warnings \nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-07T22:32:58.578922Z","iopub.execute_input":"2024-08-07T22:32:58.579314Z","iopub.status.idle":"2024-08-07T22:33:09.526662Z","shell.execute_reply.started":"2024-08-07T22:32:58.579274Z","shell.execute_reply":"2024-08-07T22:33:09.525324Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Initialize the TPU\n# resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(resolver)\n# tf.tpu.experimental.initialize_tpu_system(resolver)\n\n# # Create a distribution strategy\n# strategy = tf.distribute.experimental.TPUStrategy(resolver)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing and Exploring the Dataset ","metadata":{}},{"cell_type":"code","source":"cols_names = ['Tweet_ID','Company','Sentiment','Tweet']\ntrain_data = pd.read_csv('/kaggle/input/twitter-entity-sentiment-analysis/twitter_training.csv',names=cols_names)\nval_data = pd.read_csv('/kaggle/input/twitter-entity-sentiment-analysis/twitter_validation.csv',names=cols_names)\n\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:33:18.713555Z","iopub.execute_input":"2024-08-07T22:33:18.715254Z","iopub.status.idle":"2024-08-07T22:33:19.086583Z","shell.execute_reply.started":"2024-08-07T22:33:18.71521Z","shell.execute_reply":"2024-08-07T22:33:19.085561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:33:22.598106Z","iopub.execute_input":"2024-08-07T22:33:22.598485Z","iopub.status.idle":"2024-08-07T22:33:22.610134Z","shell.execute_reply.started":"2024-08-07T22:33:22.59846Z","shell.execute_reply":"2024-08-07T22:33:22.609194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the shapes of the datsets\nprint(f'The Shape of the Training Data are : {train_data.shape}')\nprint(f'The Shape of the Training Data are : {val_data.shape}')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:33:25.496007Z","iopub.execute_input":"2024-08-07T22:33:25.496367Z","iopub.status.idle":"2024-08-07T22:33:25.502053Z","shell.execute_reply.started":"2024-08-07T22:33:25.496341Z","shell.execute_reply":"2024-08-07T22:33:25.5008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the Unique Category for each tweet\nunique_cats = np.unique(train_data.Sentiment)\nprint(f'The Unique Sentiments in the dataset are {list(unique_cats)}')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:33:28.151854Z","iopub.execute_input":"2024-08-07T22:33:28.152248Z","iopub.status.idle":"2024-08-07T22:33:28.199962Z","shell.execute_reply.started":"2024-08-07T22:33:28.152217Z","shell.execute_reply":"2024-08-07T22:33:28.19878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the Column Tweet_ID\ntrain_data.drop('Tweet_ID',axis=1,inplace=True)\nval_data.drop('Tweet_ID',axis=1,inplace=True)\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:33:31.040717Z","iopub.execute_input":"2024-08-07T22:33:31.041127Z","iopub.status.idle":"2024-08-07T22:33:31.060927Z","shell.execute_reply.started":"2024-08-07T22:33:31.041094Z","shell.execute_reply":"2024-08-07T22:33:31.059901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Handle Missing Values**","metadata":{}},{"cell_type":"code","source":"def check_missings(df,df_name):\n    missings = df.isna().sum()\n    missings = missings[missings>0]\n    if missings.empty:\n        print(f'The {df_name} Dataset is Free of missings')\n    else:\n        print(f'The {df_name} Dataset Contains missings at columns {list(missings.index)} with values {list(missings.values)}')\n    return missings\n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:33:41.404421Z","iopub.execute_input":"2024-08-07T22:33:41.405389Z","iopub.status.idle":"2024-08-07T22:33:41.410757Z","shell.execute_reply.started":"2024-08-07T22:33:41.405351Z","shell.execute_reply":"2024-08-07T22:33:41.409628Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for Missings\ntrain_missings = check_missings(train_data,'Training')\nval_missings = check_missings(val_data,'Validation')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:33:41.759677Z","iopub.execute_input":"2024-08-07T22:33:41.760471Z","iopub.status.idle":"2024-08-07T22:33:41.781094Z","shell.execute_reply.started":"2024-08-07T22:33:41.760434Z","shell.execute_reply":"2024-08-07T22:33:41.779944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop The Missing Values From the training dataset\ntrain_data.dropna(inplace=True)\ntrain_data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:33:44.875261Z","iopub.execute_input":"2024-08-07T22:33:44.87567Z","iopub.status.idle":"2024-08-07T22:33:44.91975Z","shell.execute_reply.started":"2024-08-07T22:33:44.875638Z","shell.execute_reply":"2024-08-07T22:33:44.918787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check For Duplicates**","metadata":{}},{"cell_type":"code","source":"def clean_tweet_dataset(df, df_name):\n\n    # Check for duplicates\n    duplicates = df.duplicated()\n    print(f\"Number of duplicates for {df_name} dataset is {duplicates.sum()} rows\")\n\n    # Display the duplicate rows, if any\n    if duplicates.sum() > 0:\n        print(\"Duplicate rows:\")\n        print(df[duplicates])\n\n    # Handle duplicates by removing them\n    df_cleaned = df.drop_duplicates()\n\n    # Verify that duplicates are removed\n    print(f\"Number of rows after removing duplicates: {df_cleaned.shape[0]}\")\n\n    # Save the cleaned dataset to a new CSV file\n    return df_cleaned","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:33:49.555052Z","iopub.execute_input":"2024-08-07T22:33:49.555415Z","iopub.status.idle":"2024-08-07T22:33:49.561595Z","shell.execute_reply.started":"2024-08-07T22:33:49.55538Z","shell.execute_reply":"2024-08-07T22:33:49.56055Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check and Clean Duplicates for the training dataset\ncleaned_train_data = clean_tweet_dataset(train_data, 'Training')\ncleaned_val_data = clean_tweet_dataset(val_data, 'Validation')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:33:52.823984Z","iopub.execute_input":"2024-08-07T22:33:52.824392Z","iopub.status.idle":"2024-08-07T22:33:52.946989Z","shell.execute_reply.started":"2024-08-07T22:33:52.82436Z","shell.execute_reply":"2024-08-07T22:33:52.946001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Show the Distribution of the teets Sentiments** ","metadata":{}},{"cell_type":"code","source":"def plot_sentiment_distribution(df,df_name):\n    \"\"\"\n    Plot a bar plot and a donut chart showing the distribution of sentiments.\n\n    :param df: pandas DataFrame, the input DataFrame containing a 'Sentiment' column\n    :return: None\n    \"\"\"\n    # Count the occurrences of each sentiment\n    sentiment_counts = df['Sentiment'].value_counts()\n\n    # Create subplots: 1 row, 2 columns\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    fig.suptitle(f'Distribution For {df_name} Dataset',fontsize=20)\n\n    # Plot 1: Bar Plot\n    ax1 = axes[0]\n    sentiment_counts.plot(kind='bar', ax=ax1, color=['skyblue', 'lightgreen', 'salmon','red'])\n    ax1.set_title('Distribution of Sentiments')\n    ax1.set_xlabel('Sentiment')\n    ax1.set_ylabel('Count')\n\n    # Add value labels on top of each bar\n    for i in ax1.patches:\n        ax1.text(i.get_x() + i.get_width() / 2, i.get_height() + 5,\n                 str(int(i.get_height())), ha='center', va='bottom')\n\n    # Plot 2: Donut Chart\n    ax2 = axes[1]\n    wedges, texts, autotexts = ax2.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%',\n                                       startangle=140, colors=['skyblue', 'lightgreen', 'salmon','red'],\n                                       wedgeprops=dict(width=0.3))\n\n    for text in autotexts:\n        text.set_color('black')\n\n    ax2.set_title('Sentiment Distribution (Percentage)')\n\n    # Draw a circle at the center to make it a donut chart\n    centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n    fig.gca().add_artist(centre_circle)\n\n    # Equal aspect ratio ensures that pie is drawn as a circle.\n    ax2.axis('equal')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:33:58.171993Z","iopub.execute_input":"2024-08-07T22:33:58.172375Z","iopub.status.idle":"2024-08-07T22:33:58.183468Z","shell.execute_reply.started":"2024-08-07T22:33:58.172345Z","shell.execute_reply":"2024-08-07T22:33:58.182319Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the Distribution of the Training Dataset\nplot_sentiment_distribution(cleaned_train_data,'Training')\n\n# Plot the Distribution of the Validation Dataset\nplot_sentiment_distribution(cleaned_val_data,'Validation')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:34:01.036631Z","iopub.execute_input":"2024-08-07T22:34:01.037041Z","iopub.status.idle":"2024-08-07T22:34:02.004665Z","shell.execute_reply.started":"2024-08-07T22:34:01.03701Z","shell.execute_reply":"2024-08-07T22:34:02.003324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Process the text and Handle The Unbalanced data Problem Using NLP Augmentation Techniques","metadata":{}},{"cell_type":"markdown","source":"**ALL Functions Of Data Augmentation Including  :**\n1. POS Tagging\n2. Back Translation\n3. Synonym Replacement","metadata":{}},{"cell_type":"markdown","source":"**Function To Balance the Data by dropping samples if not using naugmentation**","metadata":{}},{"cell_type":"code","source":"def balance_by_removing(df, class_column='Sentiment'):\n    # Find the size of the smallest class\n    class_counts = df[class_column].value_counts()\n    min_class_size = class_counts.min()\n\n    # Initialize an empty DataFrame to store balanced data\n    balanced_df = pd.DataFrame()\n\n    # For each class, drop duplicates, then randomly sample 'min_class_size' unique samples and append to 'balanced_df'\n    for cls in class_counts.index:\n        if class_counts[cls] > min_class_size:\n            class_subset = df[df[class_column] == cls].drop_duplicates()\n            balanced_class_subset = class_subset.sample(n=min_class_size)\n            balanced_df = pd.concat([balanced_df, balanced_class_subset], ignore_index=True)\n        else:\n            small_class_data = df[df[class_column] == cls]\n            balanced_df = pd.concat([balanced_df, small_class_data], ignore_index=True)\n            \n    return balanced_df\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:34:09.804187Z","iopub.execute_input":"2024-08-07T22:34:09.804573Z","iopub.status.idle":"2024-08-07T22:34:09.811942Z","shell.execute_reply.started":"2024-08-07T22:34:09.804541Z","shell.execute_reply":"2024-08-07T22:34:09.810904Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###################################\n# Function to Get the POS OF a Word\n###################################\ndef get_wordnet_pos(treebank_tag):\n    \"\"\"\n    Map POS tag to first character used by wordnet.synsets()\n    \"\"\"\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return None\n\n#####################################################    \n# Function That Doing Back Translation to a senetence\n#####################################################\ndef back_translation(text, src='en', mid='fr'):\n    translator = Translator()\n    try:\n        # Translate to the intermediate language\n        translation = translator.translate(text, source_language=src ,destination_language= mid)\n        translated_text = translation.result\n\n        # Translate back to the original language\n        back_translation = translator.translate(translated_text, source_language=mid, destination_language=src)\n        back_translated_text = back_translation.result\n\n        return back_translated_text\n    except Exception as e:\n        print(f\"Error during translation: {e}\")\n        return text.lower()  # Return original text if an error occurs\n    \n####################################\n# Function Doing Synonym Replacement\n####################################\ndef synonym_replacement(sentence, n):\n    words = word_tokenize(sentence)\n    new_words = words.copy()\n    pos_tagged_words = nltk.pos_tag(words)\n    \n    random_word_list = list(set([word for word, pos in pos_tagged_words if get_wordnet_pos(pos)]))\n    random.shuffle(random_word_list)\n    num_replaced = 0\n    \n    for random_word in random_word_list:\n        try:\n            pos = get_wordnet_pos(nltk.pos_tag([random_word])[0][1])\n            synonyms = wordnet.synsets(random_word, pos=pos)\n            if synonyms:\n                synonym = random.choice(synonyms).lemmas()[0].name()\n                new_words = [synonym if word == random_word else word for word in new_words]\n                num_replaced += 1\n            if num_replaced >= n:\n                break\n        except Exception as e:\n            print(f\"Error processing word '{random_word}': {e}\")\n    \n    return ' '.join(new_words).lower()\n\n################################################################\n# Function that applies The Augmentation Techniques to Sentences\n################################################################\ndef augment_text(sentence, num_synonyms=2):\n    # Apply Back Translation\n    back_translated_sentence = back_translation(sentence)\n    \n    # Apply Synonym Replacement\n    augmented_sentence = synonym_replacement(back_translated_sentence, num_synonyms)\n    return augmented_sentence\n\n\n##############################\n# Function To Generate samples\n#############################\ndef generate_samples(df):\n    max_class_dist = df.Sentiment.value_counts().max()\n    unique_sentiment_classes = np.unique(df.Sentiment)\n    generated_dict = {'Company': [], 'Sentiment': [], 'Tweet': []}\n    \n    for cat in unique_sentiment_classes:\n        sentiment_dist = df.Sentiment.value_counts()\n        \n        if sentiment_dist[cat] < max_class_dist:\n            num_samples_to_generate = int(max_class_dist - sentiment_dist[cat])\n            cat_df = df[df.Sentiment == cat].copy()\n            \n            if cat_df.shape[0] == 0:\n                continue  # Skip if the class has no samples\n            \n            # Generate a list of unique random integers\n            random_idx = random.sample(range(cat_df.shape[0]), min(num_samples_to_generate, cat_df.shape[0]))\n            \n            for i in random_idx:\n                generated_dict['Company'].append(cat_df.iloc[i].Company)\n                generated_dict['Sentiment'].append(cat_df.iloc[i].Sentiment)\n                generated_dict['Tweet'].append(augment_text(cat_df.iloc[i].Tweet))\n    \n    generated_samples = pd.DataFrame(generated_dict)\n    return generated_samples\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:34:10.676265Z","iopub.execute_input":"2024-08-07T22:34:10.677215Z","iopub.status.idle":"2024-08-07T22:34:10.693408Z","shell.execute_reply.started":"2024-08-07T22:34:10.677177Z","shell.execute_reply":"2024-08-07T22:34:10.69238Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pre-processing functions**","metadata":{}},{"cell_type":"code","source":"#################################################\n# Function To Remove the URLs from text if exists\n#################################################\ndef remove_url_and_domains(text):\n    # Remove the URLs\n    re_url = re.compile(r'https?://\\S+|www\\.\\S+')\n    text = re_url.sub('', text)\n    \n    # Define the regex pattern to match text containing domain-like patterns\n    pattern = r'\\b(?:\\w+@\\w+\\.\\w+|\\w+\\.\\w+|\\w+\\.(?:com|tv|org|net|edu|gov|mil|int))\\b'\n    \n    # Replace matched patterns with an empty string\n    cleaned_text = re.sub(r'\\S*\\b(?:\\w+@\\w+\\.\\w+|\\w+\\.\\w+|\\w+\\.(?:com|tv|org|net|edu|gov|mil|int))\\b\\S*', '', text)\n    \n    # Remove multiple spaces resulting from removal and trim leading/trailing spaces\n    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n    \n    return cleaned_text\n\n\n############################\n# Function To Remove Emojies\n############################\ndef remove_all_emojis(text):\n    \"\"\"\n    Removes all emojis from the given text using regular expressions.\n    \"\"\"\n    # Define a regex pattern to match emojis\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # Emoticons\n        u'\\U0001F300-\\U0001F5FF'  # Symbols & Pictographs\n        u'\\U0001F680-\\U0001F6FF'  # Transport & Map Symbols\n        u'\\U0001F700-\\U0001F77F'  # Alchemical Symbols\n        u'\\U0001F780-\\U0001F7FF'  # Geometric Shapes Extended\n        u'\\U0001F800-\\U0001F8FF'  # Supplemental Arrows Extended-A\n        u'\\U0001F900-\\U0001F9FF'  # Supplemental Symbols and Pictographs\n        u'\\U0001FA00-\\U0001FA6F'  # Chess Symbols\n        u'\\U0001FA70-\\U0001FAFF'  # Symbols and Pictographs Extended-A\n        u'\\U00002702-\\U000027B0'  # Dingbats\n        u'\\U000024C2-\\U0001F251'  # Enclosed Characters\n        u'\\U0001F004-\\U0001F0CF'  # Playing Cards\n        u'\\U0001F18E'              # Specific Symbols\n        u'\\U0001F191-\\U0001F251'  # Additional Symbols\n        u'\\U0001F600-\\U0001F64F'  # Emoticons\n        u'\\U0001F300-\\U0001F5FF'  # Symbols & Pictographs\n        u'\\U0001F680-\\U0001F6FF'  # Transport & Map Symbols\n        u'\\U0001F700-\\U0001F77F'  # Alchemical Symbols\n        u'\\U0001F780-\\U0001F7FF'  # Geometric Shapes Extended\n        u'\\U0001F800-\\U0001F8FF'  # Supplemental Arrows Extended-A\n        u'\\U0001F900-\\U0001F9FF'  # Supplemental Symbols and Pictographs\n        u'\\U0001FA00-\\U0001FA6F'  # Chess Symbols\n        u'\\U0001FA70-\\U0001FAFF'  # Symbols and Pictographs Extended-A\n        u'\\U00002702-\\U000027B0'  # Dingbats\n        u'\\U000024C2-\\U0001F251'  # Enclosed Characters\n        ']+', \n        flags=re.UNICODE\n    )\n    \n    return emoji_pattern.sub(r'', text)\n\n\n###########################################################\n# Replacing some common patterns the exists in many samples \n###########################################################\n# Collectd Manually\nreplacement_dict = {\n    r'\\br\\b'         : 'are'           ,          r'\\bwkly\\b'    : 'weekly'   ,\n    r'\\bk\\b'         : 'ok'            ,          r'\\bu\\b'       : 'you'      ,\n    r'\\btkts\\b'      : 'tickets'       ,          r'\\bb\\b'       : 'be'       ,\n    r'\\baft\\b'       : 'after'         ,          r'&amp;'       : ''         ,\n    r'â€™'           : \"'\"             ,          r'\\bur\\b'      : 'your'     ,\n    r'\\bv\\b'         : 'very'          ,          r'\\bpls\\b'     : 'please'   ,\n    r'\\bc\\b'         : 'see'           ,          r'\\blar\\b'     : 'later'    ,          \n    r'\\bda\\b'        : 'the'           ,          r'frnd'        : 'friend'   ,          \n    r'\\bwat\\b'       : 'what'          ,          r'\\babt\\b'     : 'about'    ,\n    r'\\bwen\\b'       : 'when'          ,          r'\\benuff\\b'   : 'enough'   ,          \n    r'\\bn\\b'         : 'in'            ,          r'\\brply\\b'    : 'reply'    ,\n    r'\\bthk\\b'       : 'think'         ,          r'\\btot\\b'     : 'thought'  ,\n    r'\\bnite\\b'      : 'night'         ,          r'\\bnvm\\b'     : 'never mind',\n    r'\\btxt\\b'       : 'text'          ,          r'\\btxting\\b'  : 'texting'  ,\n    r'\\bgr8\\b'       : 'great'         ,          r'\\bim\\b'      : 'i am'     ,\n    r'\\b<unk>\\b'     : ''              ,          r'\\bfav\\b'     : 'favorite' ,\n    r'\\bdlvr\\b'      : 'deliver'       ,          r\"(?<=\\s)'m(?=\\s)|^'m(?=\\s)|(?<=\\s)'m$\" : 'am',\n    r'\\b\\w*\\d\\w*\\b'  : ''              ,          r\"(?<=\\s)<unk>(?=\\s)|^<unk>(?=\\s)|(?<=\\s)<unk>$\": ''           \n    \n\n}\n\n# Define the cleaning function\ndef clean_tweets(df, replacement_dict):\n    def replace_patterns(text):\n        for pattern, replacement in replacement_dict.items():\n            text = re.sub(pattern, replacement, text)\n        return text\n    \n    df['Tweet'] = df['Tweet'].apply(replace_patterns)\n    return df\n\n\n###################################################\n# Function To Remove the punctiuation from the text\n###################################################\ndef remove_punctuation_and_special_characters(text):\n    exclude = string.punctuation.replace(\"'\",'')\n    text = text.translate(str.maketrans('', '', exclude))\n    \n    # Define a regex pattern to match punctuation and special characters, including underscores\n    pattern = r\"[^\\w\\s']\"\n    \n    # Replace matched characters with an empty string\n    cleaned_text = re.sub(pattern, '', text)\n    \n    return cleaned_text\n\n\n\n\n#############################################\n# Function to correct text using SpellChecker\n#############################################\nspell = SpellChecker()\ndef correct_text(text):\n    corrected_text = []\n    for word in text.split():\n        corrected_word = spell.correction(word)\n        if corrected_word is not None:\n            corrected_text.append(corrected_word)\n    return ' '.join(corrected_text)\n\n\n###############################\n# Function To Tokenize the Text\n###############################\ndef tokenize_text(text):\n    # Check if text is None or empty\n    if text is None or text.strip() == '':\n        return []\n\n    # Remove special characters or combinations of special characters\n    if re.fullmatch(r'[^\\w\\s]', text) is not None:\n        return []\n    \n    # Tokenize text\n    tokens = word_tokenize(text)\n    \n    # Remove single-character words\n    tokens = [word for word in tokens if len(word) > 1]\n    \n    return tokens\n\n    \n    \n#############################################\n# Function To Remove Stop Words From the Text\n#############################################\nstop_words = stopwords.words('english')+[\"'s\",\"'m\",\"'t\"]\ndef remove_stopwords(text):\n    filtered_text = []\n    for word in text:\n        if word not in stop_words:\n            filtered_text.append(word)\n    return filtered_text\n\n\n################################\n# Function To Lemmatize the Text\n################################\ndef lemmatize_text(text):\n    if isinstance(text, list):\n        # If text is a list of tokens\n        doc = nlp(' '.join(text))\n        lemmas = [token.lemma_ for token in doc]\n    else:\n        # If text is a string\n        doc = nlp(text)\n        lemmas = [token.lemma_ for token in doc]\n    return lemmas\n\n\n##################################\n# Function To Plot The Word Cloud\n##################################\ndef plot_word_clouds(df,dataset_name, text_column='Tweet', class_column='Sentiment'):\n   \n    classes = df[class_column].unique()\n    num_classes = len(classes)\n    \n    # Set up the plot dimensions\n    fig, axes = plt.subplots(1, num_classes, figsize=(6 * num_classes, 6), squeeze=False)\n    fig.suptitle(f'Words Clouds For {dataset_name} Dataset',fontsize=25)\n    \n    for i, class_label in enumerate(classes):\n        # Combine all tokens for the current class\n        class_text = ' '.join(df[df[class_column] == class_label][text_column].apply(lambda x: ' '.join(x)))\n\n        # Create the word cloud\n        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(class_text)\n\n        # Plot the word cloud\n        ax = axes[0, i]\n        ax.imshow(wordcloud, interpolation='bilinear')\n        ax.axis('off')\n        ax.set_title(f'{class_label} Word Cloud',fontsize=15)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    \n#######################################################################################\n# Function to plot a Side-Bar Plot for eachcategory that shows the most frequancy words\n#######################################################################################\ndef plot_top_words(df,dataset_name, text_column, class_column, top_n=15):\n    # Combine all tokens for each class\n    class_tokens = {}\n    for sentiment_class in df[class_column].unique():\n        class_data = df[df[class_column] == sentiment_class]\n        tokens = [token for sublist in class_data[text_column] for token in sublist]\n        class_tokens[sentiment_class] = tokens\n\n    # Calculate the frequency of each word for each class\n    class_word_freq = {}\n    for sentiment_class, tokens in class_tokens.items():\n        word_freq = Counter(tokens)\n        common_words = word_freq.most_common(top_n)\n        class_word_freq[sentiment_class] = dict(common_words)\n\n    # Determine the number of rows and columns for subplots\n    num_classes = len(class_word_freq)\n    num_cols = 2\n    num_rows = math.ceil(num_classes / num_cols)\n\n    # Set up the plot dimensions\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(16, 6 * num_rows))\n    fig.suptitle(f'Frequancies for {dataset_name} Dataset', fontsize=30)\n    fig.tight_layout(pad=5.0)\n\n    # Flatten the axes array for easier iteration\n    axes = axes.flatten()\n\n    # Plot the top words for each class\n    for i, (sentiment_class, word_freq) in enumerate(class_word_freq.items()):\n        ax = axes[i]\n        sns.barplot(ax=ax, x=list(word_freq.values()), y=list(word_freq.keys()), palette=\"coolwarm_r\")\n        ax.set_title(f\"Top {top_n} words in {sentiment_class} tweets\")\n        ax.set_xlabel(\"Frequency\")\n        ax.set_ylabel(\"Words\")\n        \n        # Add value labels to each bar\n        for index, value in enumerate(list(word_freq.values())):\n            ax.text(value, index, str(value), color='black', ha=\"left\", va=\"center\")\n        \n        # Apply coolwarm color palette\n        norm = plt.Normalize(min(word_freq.values()), max(word_freq.values()))\n        sm = plt.cm.ScalarMappable(cmap=\"coolwarm_r\", norm=norm)\n        sm.set_array([])\n        cbar = plt.colorbar(sm, ax=ax)\n        cbar.set_label('Frequency')\n\n    # Remove any empty subplots\n    for j in range(i + 1, len(axes)):\n        fig.delaxes(axes[j])\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:34:13.158421Z","iopub.execute_input":"2024-08-07T22:34:13.159324Z","iopub.status.idle":"2024-08-07T22:34:13.38529Z","shell.execute_reply.started":"2024-08-07T22:34:13.159278Z","shell.execute_reply":"2024-08-07T22:34:13.384297Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(df,augment=False):\n    # Take a Shallow Copy From the Dataset\n    processed_df = df.copy()\n    \n    # Remove Emojis\n    processed_df['Tweet'] = processed_df['Tweet'].apply(remove_all_emojis)\n    \n    # Remove URLs and Domain Names\n    processed_df['Tweet'] = processed_df['Tweet'].apply(remove_url_and_domains)\n    \n    # Lowercase the Dataset\n    processed_df['Tweet'] = processed_df['Tweet'].str.lower()\n    \n    # Replace and Remove Some Common Special Patterns\n    processed_df = clean_tweets(processed_df, replacement_dict)\n    \n    # Remove Special Characters and Punctuation\n    processed_df['Tweet'] = processed_df['Tweet'].apply(remove_punctuation_and_special_characters)\n    \n    # Drop any rows where 'Tweet' is NaN\n    processed_df.dropna(subset=['Tweet'], inplace=True)\n    \n    if augment :\n        # Generate Data Samples to Balance the Data\n        generated_samples = generate_samples(processed_df)\n        balanced_df = pd.concat([processed_df, generated_samples], ignore_index=True)\n    else:\n        # Balance the dataset by dropping random samples if not augmented\n        balanced_df = balance_by_removing(processed_df)\n    \n    # Tokenization     \n    balanced_df['Tweet'] = balanced_df['Tweet'].apply(tokenize_text)\n    \n    # Stop-words Removal     \n    balanced_df['Tweet'] = balanced_df['Tweet'].apply(remove_stopwords)\n    \n    # Lemmatization\n    balanced_df['Tweet'] = balanced_df['Tweet'].apply(lemmatize_text)\n    \n    return balanced_df\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:34:13.387025Z","iopub.execute_input":"2024-08-07T22:34:13.387338Z","iopub.status.idle":"2024-08-07T22:34:13.395751Z","shell.execute_reply.started":"2024-08-07T22:34:13.387311Z","shell.execute_reply":"2024-08-07T22:34:13.394411Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Applying the Preprocessing Steps to the Datasets**","metadata":{}},{"cell_type":"code","source":"# Apply the Pre-Processing To the Training Dataset\nbalanced_train = preprocess_text(cleaned_train_data)\n\n# Apply the Pre-Processing To the Training Dataset\nbalanced_val = preprocess_text(cleaned_val_data,True)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:34:13.397093Z","iopub.execute_input":"2024-08-07T22:34:13.397385Z","iopub.status.idle":"2024-08-07T22:41:49.802278Z","shell.execute_reply.started":"2024-08-07T22:34:13.397361Z","shell.execute_reply":"2024-08-07T22:41:49.801327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the shapes of each dataset before and after pre-processing \nprint(f'The shape of the Training dataset before Dropping is : {cleaned_train_data.shape}')\nprint(f'The shape of the Training dataset after Dropping is : {balanced_train.shape}')\n\nprint(f'The shape of the Validation dataset before Augmentation is : {cleaned_val_data.shape}')\nprint(f'The shape of the Validation dataset after Augmentation is : {balanced_val.shape}')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:41:49.804146Z","iopub.execute_input":"2024-08-07T22:41:49.804457Z","iopub.status.idle":"2024-08-07T22:41:49.810148Z","shell.execute_reply.started":"2024-08-07T22:41:49.804431Z","shell.execute_reply":"2024-08-07T22:41:49.809216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the Distribution of the Training Dataset\nplot_sentiment_distribution(balanced_train,'Balanced Training')\n\n# Plot the Distribution of the Validation Dataset\nplot_sentiment_distribution(balanced_val,'Balanced Validation')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:41:49.811442Z","iopub.execute_input":"2024-08-07T22:41:49.811892Z","iopub.status.idle":"2024-08-07T22:41:50.692541Z","shell.execute_reply.started":"2024-08-07T22:41:49.811865Z","shell.execute_reply":"2024-08-07T22:41:50.691481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the Words Cloud for the Balanced Training Dataset\nplot_word_clouds(balanced_train,'Balanced Training')\n\n# Plot the Words Cloud for the Balanced Validaton Dataset\nplot_word_clouds(balanced_val,'Balanced Validation')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:41:50.693743Z","iopub.execute_input":"2024-08-07T22:41:50.69405Z","iopub.status.idle":"2024-08-07T22:42:02.942181Z","shell.execute_reply.started":"2024-08-07T22:41:50.694025Z","shell.execute_reply":"2024-08-07T22:42:02.941062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot words Clouds For the Training Dataset\nplot_top_words(balanced_train,'Balanced Training', 'Tweet', 'Sentiment')\n\n# Plot words Clouds For the Validation Dataset\nplot_top_words(balanced_val,'Balanced Validation','Tweet', 'Sentiment')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:42:02.944009Z","iopub.execute_input":"2024-08-07T22:42:02.944377Z","iopub.status.idle":"2024-08-07T22:42:06.982184Z","shell.execute_reply.started":"2024-08-07T22:42:02.944345Z","shell.execute_reply":"2024-08-07T22:42:06.981086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the training pre-processed data\nbalanced_train.to_csv('balanced_train.csv',index=False)\n\n# Save the validation pre-processed data\nbalanced_val.to_csv('balanced_val.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:42:34.859643Z","iopub.execute_input":"2024-08-07T22:42:34.860465Z","iopub.status.idle":"2024-08-07T22:42:35.188372Z","shell.execute_reply.started":"2024-08-07T22:42:34.860427Z","shell.execute_reply":"2024-08-07T22:42:35.187435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying Feature Extraction using TF-IDF and Fit The Models","metadata":{}},{"cell_type":"markdown","source":"**Libraries for Models and Feature Extraction**","metadata":{}},{"cell_type":"code","source":"balanced_train = pd.read_csv('/kaggle/working/balanced_train.csv')\nbalanced_val = pd.read_csv('/kaggle/working/balanced_val.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:42:38.796448Z","iopub.execute_input":"2024-08-07T22:42:38.796838Z","iopub.status.idle":"2024-08-07T22:42:38.93509Z","shell.execute_reply.started":"2024-08-07T22:42:38.796808Z","shell.execute_reply":"2024-08-07T22:42:38.934061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout,BatchNormalization\nfrom keras.callbacks import History\nfrom tensorflow.keras.metrics import Precision ,Recall\nfrom sklearn.utils import shuffle","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:42:57.16959Z","iopub.execute_input":"2024-08-07T22:42:57.170329Z","iopub.status.idle":"2024-08-07T22:42:57.176904Z","shell.execute_reply.started":"2024-08-07T22:42:57.170295Z","shell.execute_reply":"2024-08-07T22:42:57.175641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Join tokens into strings\nbalanced_train['Tweet'] = balanced_train['Tweet'].apply(lambda tokens: ''.join(tokens))\nbalanced_val['Tweet'] = balanced_val['Tweet'].apply(lambda tokens: ''.join(tokens))\n\n# Initialize the TF-IDF vectorizer\ntfidf = TfidfVectorizer(max_features=5000)\n\n# Fit and transform the training data\nX_train = tfidf.fit_transform(balanced_train['Tweet'])\nX_val = tfidf.transform(balanced_val['Tweet'])","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:43:04.816464Z","iopub.execute_input":"2024-08-07T22:43:04.816862Z","iopub.status.idle":"2024-08-07T22:43:05.774803Z","shell.execute_reply.started":"2024-08-07T22:43:04.816827Z","shell.execute_reply":"2024-08-07T22:43:05.77371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode Sentiment labels\nle = LabelEncoder()\ny_train = le.fit_transform(balanced_train['Sentiment'])\ny_val = le.transform(balanced_val['Sentiment'])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:43:08.760109Z","iopub.execute_input":"2024-08-07T22:43:08.760499Z","iopub.status.idle":"2024-08-07T22:43:08.777079Z","shell.execute_reply.started":"2024-08-07T22:43:08.760468Z","shell.execute_reply":"2024-08-07T22:43:08.775873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Random Forest Model**","metadata":{}},{"cell_type":"code","source":"# Initialize and train the Random Forest model\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Predict on validation set\ny_pred_rf = rf_model.predict(X_val)\n\n# Evaluate the Random Forest model\nprint(\"Random Forest Classifier Metrics:\")\nprint(f\"Accuracy: {accuracy_score(y_val, y_pred_rf)}\")\nprint(f\"F1-score: {f1_score(y_val, y_pred_rf, average='weighted')}\")\nprint(f\"Precision: {precision_score(y_val, y_pred_rf, average='weighted')}\")\nprint(f\"Recall: {recall_score(y_val, y_pred_rf, average='weighted')}\")\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_val, y_pred_rf))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_val, y_pred_rf))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:43:14.002791Z","iopub.execute_input":"2024-08-07T22:43:14.003159Z","iopub.status.idle":"2024-08-07T22:45:16.416823Z","shell.execute_reply.started":"2024-08-07T22:43:14.003132Z","shell.execute_reply":"2024-08-07T22:45:16.415701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DNN Model**","metadata":{}},{"cell_type":"code","source":"# Initialize and train the Neural Network model\nnn_model = Sequential()\nnn_model.add(Dense(512, input_dim=X_train.shape[1], activation='relu'))\nnn_model.add(Dropout(0.5))\nnn_model.add(Dense(128, activation='relu'))\nnn_model.add(Dropout(0.5))\nnn_model.add(Dense(64, activation='relu'))\nnn_model.add(Dropout(0.5))\nnn_model.add(Dense(len(le.classes_), activation='softmax'))\n\nnn_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:48:10.130318Z","iopub.execute_input":"2024-08-07T22:48:10.130715Z","iopub.status.idle":"2024-08-07T22:48:10.241461Z","shell.execute_reply.started":"2024-08-07T22:48:10.130683Z","shell.execute_reply":"2024-08-07T22:48:10.240436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Train the model and store training history\nhistory = nn_model.fit(X_train.toarray(), y_train, epochs=10, batch_size=32, validation_data=(X_val.toarray(), y_val))\n\n# Predict on validation set\ny_pred_nn = nn_model.predict(X_val.toarray())\ny_pred_nn_classes = y_pred_nn.argmax(axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:48:14.213605Z","iopub.execute_input":"2024-08-07T22:48:14.214559Z","iopub.status.idle":"2024-08-07T22:52:47.732437Z","shell.execute_reply.started":"2024-08-07T22:48:14.214517Z","shell.execute_reply":"2024-08-07T22:52:47.731172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training history\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot loss\nax1.plot(history.history['loss'], label='Train Loss')\nax1.plot(history.history['val_loss'], label='Validation Loss')\nax1.set_title('Loss')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('Loss')\nax1.legend()\n\n# Plot accuracy\nax2.plot(history.history['accuracy'], label='Train Accuracy')\nax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\nax2.set_title('Accuracy')\nax2.set_xlabel('Epochs')\nax2.set_ylabel('Accuracy')\nax2.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:53:11.744635Z","iopub.execute_input":"2024-08-07T22:53:11.74534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on validation set\ny_pred_nn = nn_model.predict(X_val.toarray())\ny_pred_nn_classes = y_pred_nn.argmax(axis=1)\n\n# Evaluate the Neural Network model\nprint(\"Neural Network Classifier Metrics:\")\nprint(f\"Accuracy: {accuracy_score(y_val, y_pred_nn_classes)}\")\nprint(f\"F1-score: {f1_score(y_val, y_pred_nn_classes, average='weighted')}\")\nprint(f\"Precision: {precision_score(y_val, y_pred_nn_classes, average='weighted')}\")\nprint(f\"Recall: {recall_score(y_val, y_pred_nn_classes, average='weighted')}\")\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_val, y_pred_nn_classes))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_val, y_pred_nn_classes))","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:47:53.867711Z","iopub.execute_input":"2024-08-07T22:47:53.868154Z","iopub.status.idle":"2024-08-07T22:47:54.139029Z","shell.execute_reply.started":"2024-08-07T22:47:53.868114Z","shell.execute_reply":"2024-08-07T22:47:54.137919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exampel Prediction**","metadata":{}},{"cell_type":"code","source":"for i in np.random.randint(0,balanced_val.shape[0],10):\n\n    # Example prediction\n    example_tweet = balanced_val.iloc[i]['Tweet']\n    example_tweet_vectorized = tfidf.transform([example_tweet])\n\n    # Predict using Random Forest\n    pred_rf = rf_model.predict(example_tweet_vectorized)\n    # Predict using Neural Network\n    pred_nn = nn_model.predict(example_tweet_vectorized.toarray())\n    pred_nn_class = pred_nn.argmax(axis=1)\n\n    print(f\"Example Tweet: {example_tweet}\")\n    print(f\"Actual Sentiment: {balanced_val.iloc[i]['Sentiment']}\")\n    print(f\"Random Forest Prediction: {le.inverse_transform(pred_rf)[0]}\")\n    print(f\"Neural Network Prediction: {le.inverse_transform(pred_nn_class)[0]}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:47:54.140902Z","iopub.execute_input":"2024-08-07T22:47:54.141261Z","iopub.status.idle":"2024-08-07T22:47:54.959931Z","shell.execute_reply.started":"2024-08-07T22:47:54.141231Z","shell.execute_reply":"2024-08-07T22:47:54.958891Z"},"trusted":true},"execution_count":null,"outputs":[]}]}